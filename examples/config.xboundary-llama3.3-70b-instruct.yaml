model:
  generation: llama3.3
  model_name: Llama-3.3-70B-Instruct
  device: auto
  load_in_8bit: false
  # Point this at your local checkpoint folder (recommended; don't commit weights).
  path: /mnt/shared-storage-user/ai4good2-share/models/meta-llama/Llama-3.3-70B-Instruct
  # Llama-3.x instruct checkpoints are commonly bf16; use "auto" if unsure.
  dtype: bfloat16
  trust_remote_code: false
  load_tokenizer: true
  # Reference generation parameters (deterministic)
  generation_config:
    do_sample: false
    temperature: 0.0
    top_p: 1.0
    top_k: 0
    repetition_penalty: 1.0
    # Llama tokenizers often have no pad token; runner will default pad_token_id=eos_token_id.

progress_equalize_evaluators: true

evaluators:
  - type: tellme
    run_name: tellme
    batch_size: 16
    token_position: -1
    layer_ratio: 0.6666
    dataset:
      name: tellme/beaver_tails_filtered
      test_path: /mnt/shared-storage-user/guojiaxuan/code/DeepScan/dataset/tellme/test.csv
    summarizer:
      type: tellme

  - type: xboundary
    run_name: x-boundary
    batch_size: 8
    max_length: 1024
    # Either specify explicit layers...
    # target_layers: [9, 18]
    # ...or comma-separated:
    # target_layers_csv: "9,18"
    # If omitted, defaults to 1/3 and 2/3 depth.
    save_metrics_json: true
    save_tsne: true
    dataset:
      name: xboundary/diagnostic
      data_dir: /mnt/shared-storage-user/guojiaxuan/code/DeepScan/dataset/xboundary
      # Match original X-Boundary default: --num_samples (per class)
      num_samples_per_class: 200
    summarizer:
      type: xboundary
      select_by: boundary_ratio
      include_both_selections: true

  - type: spin
    run_name: spin
    # Match original SPIN default: --nsamples
    nsamples: 128
    seed: 0
    q: 5e-7               # top-q ratio per dataset
    target_module: mlp    # mlp | self_attn | all
    per_layer: true
    per_module: false
    device: cuda
    save_coupling_plot: true
    dataset:
      name: spin/csv_bundle
      dataset1_name: privacy
      dataset2_name: fairness
      dataset1_path: /mnt/shared-storage-user/guojiaxuan/code/DeepScan/dataset/spin/beaver_train330k_privacy_safe_1k.csv
      dataset2_path: /mnt/shared-storage-user/guojiaxuan/code/DeepScan/dataset/spin/beaver_train330k_fairness_safe_1k.csv
    summarizer:
      type: spin
      top_layers: 5

  - type: mi-peaks
    run_name: mi-peaks
    # Use last layer by default for portability across backbones.
    target_layer: -1
    # Optional: cap how many examples to run MI over (evaluator-level cap).
    sample_num: 10
    # Activation extraction
    max_new_tokens: 512
    token_position: -1
    # MI estimator (matches MI-Peaks default)
    sigma: 50.0
    ktype: gaussian
    # Peak summary
    top_k: 20
    top_tokens: 15
    # optionally, to reuse *existing* MI-Peaks artifacts for strict reproduction:
    artifact_root: /root/code/MI-Peaks/src
    reuse_cache: false
    dataset:
      name: mi-peaks/math_train_12k
      sample_num: 10
      csv_path: /mnt/shared-storage-user/guojiaxuan/code/DeepScan/dataset/mi_peaks/math_train_12k.csv

# Run-level summarizer that aggregates all evaluators
summarizer:
  type: combined
