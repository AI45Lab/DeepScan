# LLM‑Diagnose Framework（面向开发者的 DARPA/Heilmeier 分析）

> 场景定位：为用户**自有大模型**做**安全诊断**，识别安全方向的缺陷，并为后续加固/对齐提供方向指引。  
> 主打方法：**TELLME + X‑Boundary 并列**（同一流水线、不同诊断视角）。

---

## 1) 整体产出目标（What / Who cares / How measured）

### 我们要做什么（What）
面向专业开发者提供一套**可复现、可扩展、可集成**的“安全诊断流水线”，将研究型诊断方法工程化：用统一 Runner + 注册表 + 评估器/汇总器，把“安全诊断”从零散脚本变成可持续运行的能力。

### 为什么重要（Who cares & Why now）
- **模型会持续回归**：版本迭代、数据漂移、对齐策略调整都会引入新的安全缺陷。
- **仅评测不足以加固**：很多工具擅长回答“是否不安全/不安全多少”，但开发者更需要“为什么不安全、哪里出了问题、下一步怎么改”。
- **工程闭环需求**：把“发现问题 → 定位原因 → 制定加固动作”纳入 CI/任务系统与结果归档。

### 我们交付什么（Deliverables）
- **结构化诊断结果**：JSON（run 级落盘；可选单文件输出）。
- **诊断报告**：Summarizer 生成 `summary.json / summary.md`，便于评审/复盘/归档与对比。
- **可视化与诊断线索**：X‑Boundary 可选 t‑SNE 与 separation/boundary 指标；TELLME 输出表征度量与距离/相似度类指标。
- **运行与集成方式**：CLI 批量跑；FastAPI 任务化触发与查询；可选 webhook 回传进度/结果以接入平台。

### 如何衡量成功（Metrics / “考试题”）
- **TTFR（Time to First Run）**：新用户 ≤ 10 分钟跑通安全诊断示例（含模板配置与最小样例）。
- **可比较性**：同数据同配置下，多模型/多 checkpoint 的结果可横向对比（稳定 schema + summarizer）。
- **行动指向性**：报告能指出“边界塌陷/表征纠缠/异常层”等信号，并映射到加固动作。
- **集成性**：可在 CI/任务系统周期性运行，产物可追踪（run_id + 目录/报告归档）。

---

## 2) 形态/形式（Approach / System / Transition）

### 总体形态（开发者怎么用）
`注册（模型/数据/方法） → 配置（YAML/JSON） → 执行（CLI/API） → 产物（结果 + 报告 + 图）`

### 系统组件（抽象与分工）
- **Registry**：Model/Dataset/Evaluator/Summarizer 统一注册与实例化，避免硬编码依赖。
- **Runner**：统一 `generate/chat` 屏蔽不同模型族差异，同时保留底层 `model/tokenizer` 直达以支持深度诊断。
- **ConfigLoader**：将“实验定义”和“实现”解耦，支持批量实验、迁移与 A/B 对比。
- **Summarizer**：将“算法输出”和“对外报告/指标选择/格式化”解耦，便于团队沉淀统一报告。

### 主打方法并列：TELLME + X‑Boundary（两种诊断视角）
- **TELLME（表征结构信号）**
  - 关注：安全/不安全概念是否被清晰编码、是否纠缠。
  - 产物：表征度量与距离/相似度类指标（用于判断“表征健康度”）。
- **X‑Boundary（边界分离信号）**
  - 关注：安全/不安全在表示空间的分离度与边界混淆程度。
  - 产物：separation/boundary 指标 + 可视化（用于定位“边界是否塌陷、塌陷在哪里”）。
- **组合价值**
  - TELLME 更像“体检”（结构是否健康），X‑Boundary 更像“影像”（边界是否清晰）。
  - 二者组合更利于把“发现失败样本”转化为“加固策略”。

### 迁移/落地路径（Transition）
从“脚本散落、难复现” → “配置化基线跑通” → “团队内统一诊断接口（CLI/API）” → “新增诊断方法像加插件一样可维护、可回归”。

---

## 3) 竞品（How is it done today / What are limits）

### 将 OpenAI Evals / Inspect Evals 作为竞品：可以、但要讲清边界
它们更接近 **evaluation harness（评测/回归框架）**，你的框架更偏 **diagnostic framework（诊断框架）**。两者有交集但侧重点不同，适合用“互补”叙事。

### A) OpenAI Evals / Inspect Evals（安全评测/回归）
- **强项**
  - 任务定义与 rubric（规则或 LLM‑as‑judge）。
  - 持续回归与对比，输出分数/通过率/样本级日志。
- **局限（对“安全诊断→加固”而言）**
  - 多数情况下回答“出没出事/出多少”，较难回答“为什么出事/哪里坏/从哪里加固”。
- **与你的关系（推荐表述）**
  - **Evals/Inspect**：用于发现回归与失败样本（行为层）。
  - **LLM‑Diagnose**：用于对失败样本做**表征与边界诊断**（内部信号），输出可行动线索。

> 可用于 README 的一句话：  
> **“Evals/Inspect 告诉你安全表现‘好不好’，LLM‑Diagnose 进一步告诉你‘哪里坏、边界如何塌陷、表征是否纠缠’，并把诊断方法工程化为可复现流水线，指导加固。”**

### B) lm-eval-harness 等 benchmark 框架
- **强项**：标准任务跑分与跨模型对比。
- **局限**：以 score 为中心，不聚焦表征层诊断与可视化产物。

### C) TransformerLens/Captum 等解释性工具箱
- **强项**：底层解释/归因能力深。
- **局限**：需要开发者自行拼装流程、落盘与报告；缺少“端到端诊断流水线 + 结果协议 + 批量运行形态”。

### D) Phoenix/LangSmith/W&B 等平台
- **强项**：线上追踪、标注闭环、回归治理。
- **局限**：论文级表征诊断方法与离线可控运行通常不是核心诉求；扩展方法栈可能受限。

---

## 4) 我们的优势（What’s new / Why will it succeed / Risks）

### 优势 A：从“评测”升级到“诊断→加固指引”
输出不仅是分数，而是可用于定位原因的诊断信号（分离度、边界比、表征度量、可视化），更贴近“加固决策需要的信息”。

### 优势 B：方法并列但流程统一（对开发者更友好）
TELLME 与 X‑Boundary 并列主打，但对用户而言只需在配置里切换 evaluator/summarizer；同一产物协议便于团队对比与沉淀。

### 优势 C：工程可集成（CLI + API + webhook）
既可离线批量跑，也可服务化触发任务，并把进度/结果回传到内部系统，适合做持续安全诊断基础设施。

### 优势 D：面向私有模型/私有数据的扩展成本低
通过 registry 注册私有模型与安全数据集，不改主流程；对企业/研究组现实需求更匹配。

### 关键风险与应对（DARPA 必答）
- **诊断更重（算力/时间）**  
  - 应对：小样本快速配置、分层抽样策略、可选层/层比例、可选可视化开关。
- **用户数据格式多样**  
  - 应对：提供“安全诊断数据接口规范”与转换脚本/示例。
- **指标 → 动作映射不直观**  
  - 应对：提供“加固 Playbook”（见下一节），把指标模式映射到具体加固动作。

---

## 6) 与评测框架（OpenAI Evals / Inspect Evals）的推荐协作方式
- **第一阶段：评测发现问题**  
  用 Evals/Inspect 做安全题集与回归，得到失败样本与回归版本范围。
- **第二阶段：诊断定位原因**  
  将失败样本/样本簇输入 LLM‑Diagnose，用 TELLME + X‑Boundary 输出“边界/表征”线索与异常层。
- **第三阶段：制定加固与验证**  
  形成专项数据与训练动作，重新跑 Evals/Inspect（行为层）+ LLM‑Diagnose（内部信号）双重验证。

---

## 7) 对外传播的两句话（可直接放 README）
- **定位**：从安全评测走向安全诊断——让加固有方向。  
- **对照**：Evals/Inspect 告诉你“好不好”，LLM‑Diagnose 告诉你“哪里坏、怎么修”。

